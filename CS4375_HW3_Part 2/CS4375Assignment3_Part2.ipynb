{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JKHEQf5fDUHb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import re as regex\n",
        "import random\n",
        "import math\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing"
      ],
      "metadata": {
        "id": "nWAVKeJ_cBEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_input = requests.get('https://raw.githubusercontent.com/avi20025/Machine-Learning/main/usnewshealth.txt')\n",
        "tweets_input = tweets_input.text\n",
        "tweets_input = tweets_input.split('\\n')\n",
        "\n",
        "print(len(tweets_input))"
      ],
      "metadata": {
        "id": "Ae62HRlIhweO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124b87e8-94ed-43d3-a9a3-a2fd2359efcd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_start_index = 50"
      ],
      "metadata": {
        "id": "k1Re-4z9llle"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweets(tweets):\n",
        "\n",
        "    processed_data = []\n",
        "\n",
        "    for tweets in tweets_input:\n",
        "\n",
        "        # Remove tweet id and timestamp\n",
        "        tweet = tweets[tweet_start_index:]\n",
        "\n",
        "        # Remove words starting with '@'\n",
        "        tweet = regex.sub('@[^ \\t]+', '', tweet)\n",
        "\n",
        "        # Remove hashtag symbols\n",
        "        tweet = regex.sub('#', '', tweet)\n",
        "\n",
        "        # Remove URLs\n",
        "        tweet = regex.sub('(http|https)://[^\\s]*', '', tweet)\n",
        "\n",
        "        # Convert to lowercase\n",
        "        tweet = tweet.lower()\n",
        "\n",
        "        tweet = tweet.split()\n",
        "\n",
        "        processed_data.append(tweet)\n",
        "\n",
        "    #for tweet in processed_data[:30]:\n",
        "        #print(tweet)\n",
        "\n",
        "    return processed_data"
      ],
      "metadata": {
        "id": "vnLWrOuYngiI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_distance(tweet1, tweet2):\n",
        "    # Finding count of intersection between two tweets\n",
        "    intersection = len(list(set(tweet1)&set(tweet2)))\n",
        "\n",
        "    # Finding count of union between two tweets\n",
        "    union = len(tweet1) + len(tweet2) - intersection\n",
        "\n",
        "    # Returning jaccard distance\n",
        "    jdistance = (1 - (intersection / union))\n",
        "\n",
        "    return jdistance"
      ],
      "metadata": {
        "id": "FjFb1RDIxSOR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_to_clusters(data, centroids):\n",
        "\n",
        "    cluster = dict()\n",
        "\n",
        "    tweet = 0\n",
        "    # for all tweets, search for centroids and assign it to the closest centroid\n",
        "    while tweet < len(data):\n",
        "        minimum_distance = math.inf\n",
        "        tweets = data[tweet]\n",
        "        C_index = -1\n",
        "\n",
        "        i = 0\n",
        "        # find the closest centroid\n",
        "        while i < len(centroids):\n",
        "            centroid = centroids[i]\n",
        "            distance = jaccard_distance(centroid, data)\n",
        "\n",
        "            if centroid == tweets:\n",
        "                C_index = i\n",
        "                minimum_distance = 0\n",
        "                break\n",
        "\n",
        "            if distance < minimum_distance:\n",
        "                C_index = i\n",
        "                minimum_distance = distance\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        # randomise centroid assignment to a tweet if there is nothing in common\n",
        "        if minimum_distance == 1:\n",
        "            C_index = random.randint(0, len(centroids) - 1)\n",
        "\n",
        "\n",
        "        clustertweet = cluster.setdefault(C_index, [])\n",
        "        clustertweet.append([tweets])\n",
        "\n",
        "        t_index = len(clustertweet) - 1\n",
        "        clustertweet[t_index].append(minimum_distance)\n",
        "\n",
        "        tweet += 1\n",
        "\n",
        "    return cluster"
      ],
      "metadata": {
        "id": "y6Wyht5RcN7N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_centroids(clusters):\n",
        "\n",
        "    centroids = []\n",
        "\n",
        "    i = 0\n",
        "    # iterate over each cluster to check for a tweet with closest distance sum with every other tweets present in the same cluster\n",
        "    while i < len(clusters):\n",
        "      min_dists = math.inf\n",
        "      centroid_index = -1\n",
        "\n",
        "      min_dists_arr = []\n",
        "\n",
        "      tweet1 = 0\n",
        "      while tweet1 < len(clusters[i]):\n",
        "        min_dists_arr.append([])\n",
        "        dist_sum = 0\n",
        "\n",
        "        tweet2 = 0\n",
        "        # calculates distance sum for every pair of tweets in the same cluster\n",
        "        while tweet2 < len(clusters[i]):\n",
        "          if tweet1 != tweet2:\n",
        "            if tweet2 > tweet1:\n",
        "              distance = jaccard_distance(clusters[i][tweet1][0], clusters[i][tweet2][0])\n",
        "            else:\n",
        "              distance = min_dists_arr[tweet2][tweet1]\n",
        "\n",
        "\n",
        "            min_dists_arr[tweet1].append(distance)\n",
        "            dist_sum += distance\n",
        "          else:\n",
        "            min_dists_arr[tweet1].append(0)\n",
        "\n",
        "          tweet2 += 1\n",
        "\n",
        "        # identifies and selects tweet with minimum distance sum as centroid for current cluster\n",
        "        if dist_sum < min_dists:\n",
        "          min_dists = dist_sum\n",
        "          centroid_index = tweet1\n",
        "\n",
        "        tweet1 += 1\n",
        "\n",
        "      # puts identified tweet into centroid list\n",
        "      centroids.append(clusters[i][centroid_index][0])\n",
        "\n",
        "      i += 1\n",
        "\n",
        "    return centroids"
      ],
      "metadata": {
        "id": "EXMo99AjcOsz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans(data, k , max_iter=35):\n",
        "    # Randomize the seed\n",
        "    random.seed()\n",
        "\n",
        "    oldCentroids = []\n",
        "    iteration = 0\n",
        "\n",
        "    # Initializing by randomly selecting k data as starting centroids\n",
        "    centroids = random.sample(data, k)\n",
        "\n",
        "    # iterate until max_iteration or converged\n",
        "    while (iteration < max_iter) and (not check_Converge(oldCentroids, centroids)) :\n",
        "        print(f\"Iteration number {iteration}\")\n",
        "\n",
        "        # assign data to clusters\n",
        "        clusters = assign_to_clusters(data, centroids)\n",
        "\n",
        "        # compare with previous centroids to check if converging\n",
        "        oldCentroids = centroids\n",
        "\n",
        "        # updating centroid with clusters\n",
        "        centroids = update_centroids(clusters)\n",
        "        iteration += 1\n",
        "\n",
        "    if iteration == max_iter:\n",
        "        print(\"it reached maximum number of iterations, it did not converge..\")\n",
        "    else:\n",
        "        print(\"it converged..\")\n",
        "\n",
        "    sse = calculate_sse(clusters)\n",
        "\n",
        "    return clusters, sse"
      ],
      "metadata": {
        "id": "KIhRAEbhcQcM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sse(clusters):\n",
        "    sse = 0\n",
        "\n",
        "    # iterate over clusters and compute sum of squared error of distances of tweets from its centroid\n",
        "    for i in range(len(clusters)):\n",
        "      for j in range(len(clusters[i])):\n",
        "        sse += (clusters[i][j][1] * clusters[i][j][1])\n",
        "\n",
        "    return sse"
      ],
      "metadata": {
        "id": "Ym2uHDwV-9oG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_Converge(old_centroid, new_centroids):\n",
        "    # check if the lengths are same and return false if so\n",
        "    if len(old_centroid) != len(new_centroids):\n",
        "        return False\n",
        "\n",
        "    # check if each pair of centroids are equal\n",
        "    for old, current in zip(old_centroid, new_centroids):\n",
        "        if \" \".join(current) != \" \".join(old):\n",
        "            return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "WTtdjlwxqAAC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess the tweets\n",
        "preprocess_tweets(tweets_input)\n",
        "tweets = tweets_input\n",
        "\n",
        "# initial K value for Kmeans, change if needed\n",
        "k=5\n",
        "\n",
        "print(\"K means for k = {}\".format(k))\n",
        "clusters, sse = kmeans(tweets, k)\n",
        "for c in range(len(clusters)):\n",
        "            print(str(c+1) + \": \", str(len(clusters[c])) + \" tweets\")\n",
        "print(\"SSE == {} \".format(sse))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43ZXK_7KACyn",
        "outputId": "0ee232a4-ec7f-4bcc-a97b-ab0b6e0380d5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K means for k = 5\n",
            "Iteration number 0\n",
            "Iteration number 1\n",
            "Iteration number 2\n",
            "it converged..\n",
            "1:  282 tweets\n",
            "2:  293 tweets\n",
            "3:  266 tweets\n",
            "4:  261 tweets\n",
            "5:  299 tweets\n",
            "SSE == 1396.0 \n"
          ]
        }
      ]
    }
  ]
}